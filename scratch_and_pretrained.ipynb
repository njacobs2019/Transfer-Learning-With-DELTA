{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mahsa Mozafarinia\n",
    "#### Fine tunning pre-trained Resnet-50 on Cifar-10 and training from scratch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3105,
     "status": "ok",
     "timestamp": 1680121798987,
     "user": {
      "displayName": "Mahsa Mozafarinia",
      "userId": "15056223764185142880"
     },
     "user_tz": 240
    },
    "id": "AjytVwSQp3g9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torchvision.models import (\n",
    "    resnet50,\n",
    "    ResNet50_Weights,\n",
    "    vgg11,\n",
    "    vgg16,\n",
    "    alexnet,\n",
    "    VGG11_Weights,\n",
    "    VGG16_Weights,\n",
    "    AlexNet_Weights,\n",
    ")\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "import math\n",
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import sampler\n",
    "# from utils import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = \"/Users/mahsa.mozafarinia/Documents/Jupyter/Machine Learning Project/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1680121798992,
     "user": {
      "displayName": "Mahsa Mozafarinia",
      "userId": "15056223764185142880"
     },
     "user_tz": 240
    },
    "id": "vaJ0unCrp_LL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# from EDGE_4_4_1 import EDGE\n",
    "# from matplotlib.ticker import MaxNLocator\n",
    "# plt.style.use('ggplot')\n",
    "log = logging.getLogger(\"sampleLogger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1278,
     "status": "ok",
     "timestamp": 1680122081473,
     "user": {
      "displayName": "Mahsa Mozafarinia",
      "userId": "15056223764185142880"
     },
     "user_tz": 240
    },
    "id": "IXdBpE8zyNb2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 278,
     "status": "ok",
     "timestamp": 1680122085766,
     "user": {
      "displayName": "Mahsa Mozafarinia",
      "userId": "15056223764185142880"
     },
     "user_tz": 240
    },
    "id": "pXd-a6wTqCat",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, device, arch, pretrained=False):\n",
    "        \"docstring\"\n",
    "        self.preprocess = None\n",
    "        self.model = None\n",
    "        self.arch = arch\n",
    "        self.pretrained = pretrained\n",
    "        self.device = device\n",
    "\n",
    "    def set_model(self):\n",
    "        if self.arch == \"vgg11\":\n",
    "            if self.pretrained:\n",
    "                weights = VGG11_Weights.IMAGENET1K_V1\n",
    "                self.preprocess = weights.transforms()\n",
    "                self.model = vgg11(weights=weights).to(self.device)\n",
    "                self.model.eval()\n",
    "            else:\n",
    "                self.model = vgg11().to(self.device)\n",
    "        elif self.arch == \"vgg16\":\n",
    "            if self.pretrained:\n",
    "                weights = VGG16_Weights.IMAGENET1K_V1\n",
    "                self.preprocess = weights.transforms()\n",
    "                self.model = vgg16(weights=weights).to(self.device)\n",
    "                self.model.eval()\n",
    "            else:\n",
    "                self.model = vgg16().to(self.device)\n",
    "        elif self.arch == \"resnet\":\n",
    "            if self.pretrained:\n",
    "                weights = ResNet50_Weights.DEFAULT\n",
    "                self.preprocess = weights.transforms()\n",
    "                self.model = resnet50(weights=weights).to(self.device)\n",
    "                self.model.eval()\n",
    "            else:\n",
    "                self.model = resnet50().to(self.device)\n",
    "        elif self.arch == \"alexnet\":\n",
    "            if self.pretrained:\n",
    "                weights = AlexNet_Weights.IMAGENET1K_V1\n",
    "                self.preprocess = weights.transforms()\n",
    "                self.model = alexnet(weights=weights).to(self.device)\n",
    "                self.model.eval()\n",
    "            else:\n",
    "                self.model = alexnet().to(self.device)\n",
    "        else:\n",
    "            sys.exit(\"Wrong architecture\")\n",
    "        return self.model\n",
    "\n",
    "    def trained_enough(\n",
    "        self, accuracy, dataloader, loss_fn, optimizer, epochs, device\n",
    "    ):  # I think it is not used in this code.\n",
    "        i = 0\n",
    "        while accuracy < 0.20:\n",
    "            accuracy, _ = train(\n",
    "                self.model, dataloader, loss_fn, optimizer, epochs, device\n",
    "            )\n",
    "            log.debug(f\"{i} epoch extra training, accuracy: {100 * accuracy}\")\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2PDA9jjxtUO",
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the Data with batching\n",
    "# In this part, I am going to define mean and variance of CIFAR-10\n",
    "cifar_trainset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transforms.ToTensor()\n",
    ")\n",
    "\n",
    "imgs = [\n",
    "    item[0] for item in cifar_trainset\n",
    "]  # item[0] and item[1] are image and its label\n",
    "imgs = torch.stack(imgs, dim=0).numpy()\n",
    "\n",
    "# calculate mean over each channel (r,g,b)\n",
    "mean_r = imgs[:, 0, :, :].mean()\n",
    "mean_g = imgs[:, 1, :, :].mean()\n",
    "mean_b = imgs[:, 2, :, :].mean()\n",
    "print(mean_r, mean_g, mean_b)\n",
    "\n",
    "# calculate std over each channel (r,g,b)\n",
    "std_r = imgs[:, 0, :, :].std()\n",
    "std_g = imgs[:, 1, :, :].std()\n",
    "std_b = imgs[:, 2, :, :].std()\n",
    "print(std_r, std_g, std_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the Data with mean and std of imagenet\n",
    "print(\"==> Preparing data..\")\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(\n",
    "            224\n",
    "        ),  # first crop the image randomly and then resize it.\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "        # [0.49139968, 0.48215827, 0.44653124], [0.24703233, 0.24348505, 0.26158768]\n",
    "    ]\n",
    ")\n",
    "# [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform_train\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "testset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform_test\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=128, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_func(epoch_number, scheduler_fun):\n",
    "    for sched in scheduler_fun:\n",
    "        # epoch_number=50\n",
    "        network = Network(device, \"resnet\", True)\n",
    "        model = network.set_model()\n",
    "        model.fc = nn.Linear(2048, 10)\n",
    "        # model.load_state_dict(torch.load(path+'Weights/res-CIF-iter{}-epoch99.pth'.format(itera)))\n",
    "        model = model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "        if sched == \"steplr\":\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer, step_size=10, gamma=0.1\n",
    "            )\n",
    "        elif sched == \"explr\":\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "                optimizer, gamma=math.exp(math.log(0.1) / 10)\n",
    "            )\n",
    "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epoch_number)\n",
    "        # train_test_accuracy_epochs=pd.read_csv(path+'Accuracies/res_CIF_epoch_accs-iter{}.csv'.format(itera),index_col=0)\n",
    "\n",
    "        train_test_accuracy_epochs = pd.DataFrame(\n",
    "            [[0 for i in range(epoch_number)], [0 for i in range(epoch_number)]],\n",
    "            columns=[i for i in range(epoch_number)],\n",
    "        )\n",
    "\n",
    "        for epoch in range(0, epoch_number):\n",
    "            # training\n",
    "            print(\"\\nEpoch: %d\" % epoch)\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            #         print(epoch, 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #                          % (train_loss/len(trainloader), 100.*correct/total, correct, total))\n",
    "            # saving epoch train in this iter in a dataframe to plot later.\n",
    "            train_test_accuracy_epochs.loc[:, epoch] = None\n",
    "            train_test_accuracy_epochs.iloc[0, epoch] = 100.0 * correct / total\n",
    "\n",
    "            # testing\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                    test_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    test_total += targets.size(0)\n",
    "                    test_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            scheduler.step()\n",
    "            print(\"\\n______________lr____________\\n\", optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "            # print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            # print(epoch, 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #                      % (test_loss/len(testloader), 100.*test_correct/test_total, test_correct, test_total))\n",
    "\n",
    "            train_test_accuracy_epochs.iloc[1, epoch] = (\n",
    "                100.0 * test_correct / test_total\n",
    "            )\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                path\n",
    "                + \"Weights/res-CIF-iter{}-epoch{}-scheduler{}.pth\".format(\n",
    "                    itera, epoch, sched\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                epoch,\n",
    "                \"test-Loss: %.3f | Acc: %.3f%% (%d/%d)\"\n",
    "                % (\n",
    "                    test_loss / len(testloader),\n",
    "                    100.0 * test_correct / test_total,\n",
    "                    test_correct,\n",
    "                    test_total,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                epoch,\n",
    "                \"train-Loss: %.3f | Acc: %.3f%% (%d/%d)\"\n",
    "                % (\n",
    "                    train_loss / len(trainloader),\n",
    "                    100.0 * correct / total,\n",
    "                    correct,\n",
    "                    total,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        train_test_accuracy_epochs.to_csv(\n",
    "            path\n",
    "            + \"Accuracies/res_CIF_epoch_accs-iter{}-scheduler{}.csv\".format(\n",
    "                itera, sched\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_func(50, [\"explr\", \"steplr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading Data using mean and variance of CIFAR-10\n",
    "print(\"==> Preparing data..\")\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(\n",
    "            224\n",
    "        ),  # first crop the image randomly and then resize it.\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.49139968, 0.48215827, 0.44653124], [0.24703233, 0.24348505, 0.26158768]\n",
    "        ),\n",
    "        # [0.49139968, 0.48215827, 0.44653124], [0.24703233, 0.24348505, 0.26158768]\n",
    "    ]\n",
    ")\n",
    "# [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            [0.49139968, 0.48215827, 0.44653124], [0.24703233, 0.24348505, 0.26158768]\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "trainset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform_train\n",
    ")\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=128, shuffle=True, num_workers=2\n",
    ")\n",
    "\n",
    "testset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform_test\n",
    ")\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=128, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "classes = (\n",
    "    \"plane\",\n",
    "    \"car\",\n",
    "    \"bird\",\n",
    "    \"cat\",\n",
    "    \"deer\",\n",
    "    \"dog\",\n",
    "    \"frog\",\n",
    "    \"horse\",\n",
    "    \"ship\",\n",
    "    \"truck\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_func(epoch_number, scheduler_fun):\n",
    "    for sched in scheduler_fun:\n",
    "        # best_acc = 0  # best test accurac\n",
    "        network = Network(device, \"resnet\", False)\n",
    "        model = network.set_model()\n",
    "        model.fc = nn.Linear(2048, 10)\n",
    "        # model.load_state_dict(torch.load(path+'Weights/res-CIF-iter{}-epoch99.pth'.format(itera)))\n",
    "        model = model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        if sched == \"steplr\":\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer, step_size=40, gamma=0.1\n",
    "            )\n",
    "        elif sched == \"explr\":\n",
    "            scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
    "                optimizer, gamma=math.exp(math.log(0.1) / 40)\n",
    "            )\n",
    "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epoch_number)\n",
    "        # train_test_accuracy_epochs=pd.read_csv(path+'Accuracies/res_CIF_epoch_accs-iter{}.csv'.format(itera),index_col=0)\n",
    "\n",
    "        train_test_accuracy_epochs = pd.DataFrame(\n",
    "            [[0 for i in range(epoch_number)], [0 for i in range(epoch_number)]],\n",
    "            columns=[i for i in range(epoch_number)],\n",
    "        )\n",
    "\n",
    "        for epoch in range(0, epoch_number):\n",
    "            # training\n",
    "            print(\"\\nEpoch: %d\" % epoch)\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            #         print(epoch, 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #                          % (train_loss/len(trainloader), 100.*correct/total, correct, total))\n",
    "            # saving epoch train in this iter in a dataframe to plot later.\n",
    "            train_test_accuracy_epochs.loc[:, epoch] = None\n",
    "            train_test_accuracy_epochs.iloc[0, epoch] = 100.0 * correct / total\n",
    "\n",
    "            # testing\n",
    "            model.eval()\n",
    "            test_loss = 0\n",
    "            test_correct = 0\n",
    "            test_total = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                    test_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    test_total += targets.size(0)\n",
    "                    test_correct += predicted.eq(targets).sum().item()\n",
    "\n",
    "            scheduler.step()\n",
    "            print(\"\\n______________lr____________\\n\", optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "            # print(batch_idx, len(testloader), 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #              % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))\n",
    "            # print(epoch, 'Loss: %.3f | Acc: %.3f%% (%d/%d)'\n",
    "            #         ,           % (test_loss/len(testloader), 100.*test_correct/test_total, test_correct, test_total))\n",
    "\n",
    "            train_test_accuracy_epochs.iloc[1, epoch] = (\n",
    "                100.0 * test_correct / test_total\n",
    "            )\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                path\n",
    "                + \"Weights/res-CIF-iter-epoch{}_scratch-scheduler{}-step20.pth\".format(\n",
    "                    epoch, sched\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                epoch,\n",
    "                \"test Loss: %.3f | Acc: %.3f%% (%d/%d)\"\n",
    "                % (\n",
    "                    test_loss / len(testloader),\n",
    "                    100.0 * test_correct / test_total,\n",
    "                    test_correct,\n",
    "                    test_total,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            print(\n",
    "                epoch,\n",
    "                \"train Loss: %.3f | Acc: %.3f%% (%d/%d)\"\n",
    "                % (\n",
    "                    train_loss / len(trainloader),\n",
    "                    100.0 * correct / total,\n",
    "                    correct,\n",
    "                    total,\n",
    "                ),\n",
    "            )\n",
    "\n",
    "        train_test_accuracy_epochs.to_csv(\n",
    "            path\n",
    "            + \"Accuracies/res_CIF_epoch_accs_scratch_scheduler{}-step20.csv\".format(\n",
    "                sched\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I run this model using different step size. When the step size was less than 40, learning rate gets small very fast, results in vary small steps towards minimum. As the number of epochs we used in all our expriments are 50, this small steps leads the model to have around 80 percent accuracy on both test and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_func(50, [\"steplr\", \"explr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNRC6e1JxQ7NeA/G4LD3cl3",
   "mount_file_id": "1iYvv2FcNVD4Sw_CAwqFwIsOmtgPlUaTU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
